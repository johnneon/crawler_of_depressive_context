# Обучение модели для определения депрессивного контекста

## Архитектура модели

В рамках данного исследования была обучена нейросетевая модель для анализа текстовых данных на предмет наличия депрессивного контекста. Выбрана двухслойная LSTM архитектура с применением предварительно обученных FastText эмбеддингов (cc.ru.300.bin) для русскоязычных текстов. Данная конфигурация была выбрана ввиду эффективности рекуррентных нейронных сетей при работе с последовательными данными, такими как текст.

## Гиперпараметры обучения

Процесс обучения модели осуществлялся со следующими гиперпараметрами:
- Размерность скрытого слоя: 128
- Количество слоев LSTM: 2
- Вероятность dropout: 0.5
- Скорость обучения: 0.001
- Количество эпох: 20
- Размер партии (batch size): 32

## Инициализация весов

Для улучшения сходимости и стабилизации процесса обучения применялись специальные методы инициализации весов, подобранные для конкретных типов слоев модели:

1. **Xavier инициализация** для линейных слоев (nn.Linear) — распределяет начальные значения весов равномерно в диапазоне, зависящем от размера входного и выходного слоя. Это позволяет поддерживать дисперсию сигналов при прямом и обратном проходе, что предотвращает затухание или взрыв градиентов в глубоких сетях. Смещения (bias) инициализировались нулями для уменьшения начального смещения модели.

2. **Ортогональная инициализация** для LSTM слоев — начальные матрицы весов формируются как ортогональные, что особенно важно для рекуррентных сетей. Ортогональные матрицы имеют собственные значения с модулем равным единице, что помогает предотвратить проблему исчезающих или взрывающихся градиентов при обработке длинных последовательностей. Применение этого метода значительно улучшает сходимость рекуррентных моделей.

3. **Константная инициализация** для слоев батч-нормализации (BatchNorm) — весовые коэффициенты инициализируются единицами, а смещения нулями, что обеспечивает "нейтральное" начальное поведение нормализованных слоев в начале обучения.

Такой подход к инициализации весов решает несколько ключевых проблем, возникающих при обучении глубоких нейронных сетей:
- Ускоряет процесс сходимости, позволяя модели быстрее достичь оптимальных значений
- Значительно снижает вероятность застревания в плохих локальных минимумах функции потерь
- Предотвращает проблему исчезающих или взрывающихся градиентов, особенно актуальную для LSTM архитектур
- Обеспечивает более стабильную динамику обучения, особенно на ранних эпохах

Правильная инициализация весов является важным дополнением к другим техникам оптимизации, применяемым при обучении модели, и существенно влияет на итоговое качество модели и скорость ее обучения.

## Механизм внимания

В разработанной модели используется механизм внимания (attention mechanism), который играет ключевую роль в обработке текстовых данных и значительно улучшает способность модели выявлять депрессивный контекст. Механизм внимания реализован в методе `attention_net` и работает следующим образом:

1. **Вычисление весов внимания** — для каждого слова (точнее, для каждого состояния выхода LSTM) вычисляется скалярный вес, отражающий его важность для классификации:
   ```python
   attn_scores = self.attention_weights(lstm_output)
   ```
   Где `lstm_output` имеет размерность [batch_size, seq_len, hidden_dim*2] (двойная размерность из-за двунаправленности LSTM), а `attention_weights` — это линейный слой, который проецирует вектор признаков каждого слова в скалярную оценку важности.

2. **Масштабирование для стабилизации градиентов** — применяется техника масштабирования весов внимания, аналогичная используемой в архитектуре Transformer:
   ```python
   attn_scores = attn_scores / torch.sqrt(torch.tensor(self.hidden_dim * 2, dtype=torch.float32))
   ```
   Это деление на квадратный корень из размерности предотвращает слишком большие значения градиентов и стабилизирует процесс обучения.

3. **Нормализация весов с помощью softmax** — преобразование оценок внимания в вероятностное распределение:
   ```python
   attn_weights = F.softmax(attn_scores, dim=1)
   ```
   Это обеспечивает сумму весов, равную 1, по всей последовательности. Размерность `attn_weights` равна [batch_size, seq_len, 1].

4. **Взвешенное суммирование** — получение взвешенного представления текста путем матричного умножения:
   ```python
   context = torch.bmm(attn_weights.transpose(1, 2), lstm_output)
   ```
   Операция `torch.bmm` (batch matrix multiplication) выполняет пакетное умножение матриц. После транспонирования `attn_weights` получает размерность [batch_size, 1, seq_len], что позволяет выполнить умножение на `lstm_output` размерностью [batch_size, seq_len, hidden_dim*2]. Результатом является матрица `context` размерностью [batch_size, 1, hidden_dim*2].

5. **Финальное преобразование** — убирается лишняя размерность:
   ```python
   return context.squeeze(1)
   ```
   В результате получается тензор размерностью [batch_size, hidden_dim*2], который представляет собой взвешенное представление всего текста с акцентом на наиболее значимых для классификации элементах.

Механизм внимания даёт модели следующие преимущества:
- **Способность фокусироваться на важных частях текста** — модель учится выделять слова и фразы, наиболее характерные для депрессивного контекста
- **Улучшение интерпретируемости** — веса внимания можно визуализировать, чтобы понять, какие части текста были наиболее значимы для принятия решения
- **Эффективная обработка длинных последовательностей** — даже при длинных текстах механизм позволяет сохранять наиболее важную информацию
- **Адаптивное агрегирование признаков** — в отличие от простого усреднения или взятия последнего состояния LSTM, механизм внимания адаптивно сочетает информацию из разных частей последовательности

После применения механизма внимания полученное текстовое представление дополнительно нормализуется с помощью слоя `LayerNorm`, что стабилизирует обучение и улучшает генерализацию модели.

## Взвешивание классов

Учитывая существенный дисбаланс классов в данных (соотношение положительных к отрицательным примерам примерно 1:13 даже после аугментации), для эффективного обучения модели был применен механизм взвешивания классов. Этот подход реализован с помощью функции `get_pos_weight` и применения взвешенной функции потерь `BCEWithLogitsLoss`.

```python
def get_pos_weight(labels: list) -> Tuple[torch.Tensor, torch.device]:
    """
    Рассчитывает веса классов для несбалансированных данных
    """
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    # Подсчет количества экземпляров каждого класса
    num_pos = sum(labels)
    num_neg = len(labels) - num_pos
    pos_weight = torch.tensor([num_neg / num_pos], dtype=torch.float32).to(device)
    
    return pos_weight, device
```

Суть метода заключается в следующем:
1. Вычисляется соотношение между количеством отрицательных (`num_neg`) и положительных (`num_pos`) примеров
2. Это соотношение используется в качестве веса положительного класса в функции потерь
3. Функция потерь `BCEWithLogitsLoss` с параметром `pos_weight` назначает больший штраф за ошибки классификации положительных примеров (с депрессивным контекстом)

```python
pos_weight, _ = get_pos_weight(labels)
criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)
```

Данная техника решает несколько ключевых проблем при работе с несбалансированными данными:
- **Предотвращает смещение модели** в сторону преобладающего (отрицательного) класса
- **Повышает чувствительность** к миноритарному (положительному) классу
- **Улучшает метрики Recall и F1**, что критически важно при выявлении депрессивного контекста
- **Не требует полной балансировки данных**, сохраняя естественное распределение классов

В нашем случае, учитывая соотношение классов примерно 1:13, значение `pos_weight` составило около 13, что означает, что ошибки классификации положительных примеров (тексты с депрессивным контекстом) штрафовались в 13 раз сильнее, чем ошибки классификации отрицательных примеров.

Комбинация взвешивания классов с другими техниками (аугментация, частичная балансировка методом partial_SMOTE) позволила эффективно справиться с проблемой дисбаланса классов и достичь высоких метрик на тестовой выборке даже для миноритарного класса.

## Оптимизация процесса обучения

В процессе обучения применялись следующие техники оптимизации:

1. Обрезка градиентов (градиентный клиппинг) со значением 1.0, что способствовало стабилизации процесса обучения и предотвращало проблему взрывающихся градиентов, характерную для рекуррентных нейронных сетей.

2. Планировщик скорости обучения (learning rate scheduler) для динамической корректировки параметра скорости обучения в зависимости от прогресса обучения, что позволяет более эффективно приближаться к оптимальным значениям весов модели.

3. Процедура ранней остановки (early stopping) с параметром patience=3, которая прекращает обучение, если в течение трех последовательных эпох не наблюдается улучшение целевой метрики на валидационной выборке, что помогает предотвратить переобучение.

## Динамика обучения

Общее время обучения модели составило 13 часов 38 минут, что является приемлемым с учетом объема данных и сложности задачи. Анализ динамики обучения демонстрирует стабильное улучшение всех метрик на протяжении 20 эпох тренировки:

- Потери на обучающей выборке уменьшились с 0.7882 на первой эпохе до 0.0735 на последней эпохе
- Точность (accuracy) увеличилась с 0.9495 до 0.9873
- Метрика F1 выросла с 0.6937 до 0.9128
- Значение AUC повысилось с 0.9599 до 0.9869

Такая динамика свидетельствует о высокой способности модели к обобщению и отсутствии явных признаков переобучения.

## Результаты обучения

Финальная модель продемонстрировала высокие показатели эффективности на тестовой выборке:
- Точность (accuracy): 0.9873
- Precision: 0.8986
- Recall: 0.9274
- F1-мера: 0.9128
- AUC: 0.9869
- Функция потерь: 0.3935

## Преимущества и недостатки модели

### Преимущества

Модель демонстрирует высокую эффективность при идентификации депрессивного контекста, о чем свидетельствуют высокие значения метрик precision и recall. Это особенно важно в контексте задачи выявления потенциально негативного контента, где ложноотрицательные результаты могут иметь серьезные последствия. Высокое значение AUC (0.9869) указывает на способность модели эффективно разделять классы, что является ключевым показателем в условиях несбалансированных данных.

Применение предварительно обученных эмбеддингов FastText позволило эффективно использовать семантическую информацию, содержащуюся в русскоязычных текстах, что особенно важно при работе с лингвистическими нюансами, связанными с выражением депрессивных состояний.

### Недостатки

Несмотря на высокие показатели эффективности, модель имеет ряд ограничений:

1. Показатель precision (0.8986), хотя и высокий, указывает на наличие ложноположительных срабатываний, что может приводить к некорректной классификации нейтральных текстов как депрессивных. В контексте практического применения это может требовать дополнительной верификации результатов.

2. Наблюдается некоторая нестабильность в динамике метрик в процессе обучения, что может указывать на чувствительность модели к особенностям отдельных партий данных и потенциальные проблемы с локальными минимумами функции потерь.

3. Архитектура LSTM, хотя и эффективна для обработки последовательных данных, имеет ограничения в способности улавливать долгосрочные зависимости в тексте, особенно в случаях с длинными последовательностями.

## Заключение

Разработанная модель представляет собой эффективный инструмент для анализа текстовых данных на предмет наличия депрессивного контекста. Достигнутые показатели эффективности свидетельствуют о высоком потенциале применения данной модели в практических задачах мониторинга контента и выявления потенциально проблемных текстов.

Дальнейшие исследования могут быть направлены на применение более сложных архитектур, таких как трансформеры, увеличение объема и разнообразия обучающих данных, а также на разработку более тонких методов регуляризации модели для повышения ее устойчивости и точности. 