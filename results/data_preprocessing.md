# Предобработка данных для обучения модели определения депрессивного контекста

## Методология предобработки

В рамках данного исследования была проведена комплексная предобработка текстовых данных для последующего обучения нейросетевой модели, анализирующей наличие депрессивного контекста. Предобработка играет ключевую роль в подготовке качественного набора данных, что напрямую влияет на эффективность итоговой модели.

## Исходные данные и их дисбаланс

Исходный набор данных характеризовался критическим дисбалансом классов. Первоначальное распределение включало всего 1327 положительных образцов (тексты с депрессивным контекстом) и 84587 отрицательных образцов (тексты без признаков депрессии), что представляет соотношение примерно 1:64. Столь выраженный дисбаланс является критическим фактором, способным негативно повлиять на процесс обучения модели и привести к сильному смещению в сторону доминирующего класса.

## Методы аугментации данных

Для уменьшения степени дисбаланса была применена аугментация положительных примеров, в результате которой добавлено 5218 синтетических образцов депрессивного контекста. Этот подход позволил увеличить размер миноритарного класса и получить более репрезентативную выборку для обучения.

Аугментация текстовых данных реализована в функции `augment_text()` и выполнялась с использованием двух основных методов:

1. **Удаление случайных слов** — из исходного текста удалялось небольшое количество случайно выбранных слов (10% от общего числа слов в тексте). Для каждого положительного примера создавалось две вариации с разными наборами удаленных слов. Это позволяет модели лучше понимать суть депрессивного контекста, не опираясь на отдельные слова.

2. **Перемешивание порядка некоторых слов** — порядок небольшого числа случайно выбранных слов (около 10% от общего числа) изменялся в пределах текста. Для каждого положительного примера создавалось две вариации с разными перестановками. Такой подход помогает модели быть более устойчивой к вариациям в порядке слов и фокусироваться на смысловом содержании, а не на структуре предложений.

Важно отметить, что текстовые примеры длиной менее 4 слов не подвергались аугментации, чтобы избежать искажения смысла. Для каждого положительного примера генерировалось до 4 дополнительных вариаций (2 с удалением слов и 2 с перемешиванием), что объясняет увеличение количества положительных примеров с 1327 до 6545.

Перед аугментацией тексты проходили предварительную обработку, включающую:
- Очистку от HTML-тегов, ссылок, упоминаний и хештегов
- Приведение к нижнему регистру
- Удаление стоп-слов с использованием расширенного словаря (стандартные стоп-слова NLTK для русского языка + специфические стоп-слова для социальных сетей)
- Лемматизацию с помощью библиотеки pymorphy3

После аугментации распределение классов включало 6545 положительных и 84587 отрицательных образцов, что соответствует соотношению примерно 1:13.

## Методы балансировки данных

Для дальнейшей борьбы с проблемой несбалансированности классов был использован метод "partial_SMOTE" (Synthetic Minority Over-sampling Technique). Данный подход позволяет частично выровнять распределение классов путем генерации синтетических примеров для миноритарного класса и одновременного сокращения числа образцов мажоритарного класса.

В результате применения данной техники обучающая выборка была трансформирована и стала содержать 5236 положительных и 67669 отрицательных примеров, сохраняя соотношение классов примерно на уровне 1:13. Хотя дисбаланс все еще присутствует, его степень была скорректирована для создания более стабильных условий обучения модели.

## Обработка выбросов и дополнительные техники

В процессе предобработки данных был применен механизм обработки выбросов (handle_outliers), позволяющий идентифицировать и соответствующим образом обрабатывать аномальные образцы, которые могут негативно влиять на обобщающую способность модели.

Дополнительная аугментация положительных примеров (augment_positive) в комбинации с обработкой выбросов способствовала повышению разнообразия обучающих данных и улучшению общего качества набора данных для обучения.

## Подготовка входных данных для модели

Для эффективного обучения нейронной сети были определены следующие параметры предобработки текстовых данных:
- Максимальная длина входной последовательности: 500 токенов
- Коэффициент дисбаланса классов: 0.333

Текстовые данные были преобразованы в числовые представления с использованием предварительно обученных FastText эмбеддингов (cc.ru.300.bin) для русскоязычных текстов. Данный подход позволяет эффективно преобразовывать слова в многомерные векторы, сохраняя их семантические свойства и взаимосвязи, что особенно важно при работе с лингвистическими нюансами, связанными с выражением депрессивных состояний.

## Разделение данных

Набор данных был разделен на обучающую и тестовую выборки в соотношении 80% к 20% (test_size: 0.2), что является стандартной практикой при подготовке данных для машинного обучения. Данное разделение позволяет оценить обобщающую способность модели на независимом наборе данных, который не использовался при обучении.

## Визуализация данных

Для лучшего понимания характеристик обрабатываемого текстового корпуса были сгенерированы облака слов (wordclouds) для каждого класса, визуализирующие наиболее часто встречающиеся термины в текстах с депрессивным контекстом и без него. Этот шаг предобработки позволяет глубже проанализировать лексические особенности каждого класса.

## Заключение по предобработке

Применённые методы предобработки данных позволили преодолеть значительный исходный дисбаланс классов (1:64) и создать более сбалансированный набор данных для обучения нейронной сети. Комбинация техник аугментации, балансировки классов и обработки выбросов способствовала улучшению качества входных данных, что является фундаментальным фактором для построения эффективной модели классификации текстов с депрессивным контекстом. 