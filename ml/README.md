# Модуль для анализа депрессивного контекста

Данный модуль содержит компоненты для обучения и использования модели глубокого обучения для определения признаков депрессии на основе текстовых данных пользователей.

## Возможности модуля

1. **Комплексная предобработка текста**:
   - Фильтрация стоп-слов с использованием NLTK и специфических для социальных сетей терминов
   - Обработка метаданных и выбросов для повышения качества входных данных
   - Эффективная очистка текста с удалением упоминаний и хештегов

2. **Балансировка несбалансированных данных**:
   - Использование методов SMOTE и ADASYN для создания синтетических примеров
   - Поддержка режимов частичной балансировки для сохранения определенной степени дисбаланса
   - Настраиваемое соотношение между классами для оптимального распределения

3. **Продвинутая архитектура нейронной сети**:
   - Двунаправленная LSTM с механизмом внимания для обработки текста
   - Отдельный модуль обработки метаданных пользователя
   - Объединение текстовых и метаданных признаков для финальной классификации

4. **Оптимизация процесса обучения**:
   - Динамическая корректировка скорости обучения (Learning Rate Scheduler)
   - Градиентный клиппинг для стабилизации обучения
   - Механизм ранней остановки для предотвращения переобучения

5. **Визуализация и анализ**:
   - Генерация облаков слов для каждого класса
   - Выделение и визуализация ключевых терминов
   - Сравнительный анализ распределения слов между классами

## Структура модуля

- **preprocessing.py** - функции для предобработки текстовых данных
- **dataset.py** - класс датасета и функции для подготовки данных
- **model.py** - архитектура нейронной модели
- **balancing.py** - функции для балансировки данных
- **training.py** - функции для обучения и оценки модели
- **prediction.py** - класс для получения предсказаний
- **train.py** - скрипт для запуска обучения модели
- **predict.py** - скрипт для получения предсказаний
- **visualization.py** - визуализация результатов
- **visualize_data.py** - анализ исходных данных

## Предварительные требования

### Зависимости

Установите все необходимые зависимости:

```bash
pip install -r requirements.txt
```

### FastText модель

Для работы с текстом необходима предобученная модель FastText для русского языка `cc.ru.300.bin`. Эта модель содержит векторные представления слов, обученные на Common Crawl и Wikipedia для русского языка.

#### Где скачать:

1. Официальный сайт FastText: https://fasttext.cc/docs/en/crawl-vectors.html
2. Прямая ссылка: https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.ru.300.bin.gz

После скачивания необходимо распаковать файл:
```bash
gzip -d cc.ru.300.bin.gz
```

Поместите файл `cc.ru.300.bin` в корневую директорию проекта или укажите путь к нему при запуске скриптов через параметр `--fasttext_path`.

## Обучение модели

Для обучения модели выполните следующую команду:

```bash
python -m ml.train --data_path dataset/data.json --fasttext_path cc.ru.300.bin --model_dir models --model_name depression_model.pt --epochs 20 --batch_size 32 --balance_method partial_smote
```

Для использования всех доступных оптимизаций:

```bash
python -m ml.train --data_path dataset/data.json --fasttext_path cc.ru.300.bin --epochs 20 --handle_outliers --augment_positive --lstm_layers 2 --dropout 0.5 --balance_method partial_smote --imbalance_ratio 0.333 --clip_grad_value 1.0 --generate_wordclouds
```

### Параметры для обучения

- `--data_path` - путь к файлу с данными
- `--fasttext_path` - путь к модели FastText
- `--model_dir` - директория для сохранения моделей
- `--model_name` - имя файла модели
- `--epochs` - количество эпох обучения
- `--batch_size` - размер батча
- `--hidden_dim` - размерность скрытого состояния LSTM
- `--dropout` - вероятность дропаута
- `--learning_rate` - скорость обучения
- `--patience` - количество эпох без улучшения до остановки (по умолчанию 3)
- `--test_size` - доля данных для валидации
- `--balance_method` - метод балансировки данных (`none`, `random_oversample`, `random_undersample`, `smote`, `adasyn`, `partial_smote`, `partial_adasyn`)
- `--imbalance_ratio` - для частичной балансировки, соотношение между позитивным и негативным классами (например, 0.333 = соотношение 1:3)
- `--max_len` - максимальная длина последовательности
- `--use_scheduler` - использовать планировщик скорости обучения
- `--output_dir` - директория для результатов
- `--visualize_dir` - директория для сохранения визуализаций
- `--lstm_layers` - количество слоёв LSTM (по умолчанию 2)
- `--handle_outliers` - обрабатывать выбросы в метаданных
- `--augment_positive` - аугментировать положительные примеры
- `--clip_grad_value` - значение для ограничения градиентов
- `--generate_wordclouds` - генерировать облака слов для анализа классов

## Получение предсказаний

Для получения предсказаний с использованием обученной модели:

```bash
python -m ml.predict --model_path models/depression_model.pt --fasttext_path cc.ru.300.bin --input_file new_users.json --output_file results/predictions.json
```

### Параметры для предсказания

- `--model_path` - путь к обученной модели
- `--fasttext_path` - путь к модели FastText
- `--input_file` - путь к файлу с пользователями (JSON)
- `--output_file` - путь для сохранения предсказаний
- `--csv_output` - путь для сохранения предсказаний в формате CSV
- `--device` - устройство для вычислений (`cpu`, `cuda`)

## Визуализация данных

Для визуализации облаков слов и распределения слов для разных классов:

```bash
python -m ml.visualize_data --data_path dataset/data.json --output_dir visualizations --generate_wordclouds --analyze_top_words --top_n 30
```

## Использование в коде

Для использования модуля в своем коде можно использовать класс `DepressionPredictor`:

```python
from ml.prediction import DepressionPredictor

# Инициализация предсказателя
predictor = DepressionPredictor(
    model_path='models/depression_model.pt',
    fasttext_path='cc.ru.300.bin'
)

# Предсказание по тексту
text = "Мне кажется, что жизнь потеряла смысл. Ничего не приносит радости."
meta_data = {
    "sex": 1,                  # пол
    "followers_count": 150,    # количество подписчиков
    "alcohol": 2,              # отношение к алкоголю
    "smoking": 1,              # отношение к курению
    "life_main": 3,            # главное в жизни
    "people_main": 2           # главное в людях
}

result = predictor.predict_from_text(text, meta_data)
print(f"Вероятность депрессии: {result['probability']:.2f}")
print(f"У пользователя депрессия: {'Да' if result['has_depression'] else 'Нет'}")
```

## Архитектура модели

Модель для определения депрессивного контекста построена на основе BiLSTM с механизмом внимания и дополнительной обработкой метаданных пользователя. Архитектура включает в себя следующие компоненты:

### Обработка текста
Текстовые данные проходят через несколько последовательных этапов обработки:
1. Преобразование слов во входных текстах в 300-мерные векторные представления с помощью предобученной модели FastText
2. Обработка полученных последовательностей через двунаправленную LSTM сеть с двумя слоями и скрытым состоянием размерностью 128
3. Применение механизма внимания, который определяет важность различных частей текста и формирует взвешенное представление
4. Нормализация полученного представления с помощью LayerNorm для стабилизации обучения

### Обработка метаданных
Метаданные пользователя обрабатываются через специальный энкодер метаданных (MetaFeaturesEncoder):
1. Входные метаданные (пол, количество подписчиков, отношение к алкоголю, курению и другие характеристики)
2. Последовательность из четырех блоков, каждый из которых включает линейное преобразование, батч-нормализацию, функцию активации ReLU и дропаут
3. Размерности последовательно уменьшаются: 128 → 64 → 32 → 16, что позволяет извлечь наиболее значимые признаки

### Классификация
Признаки, полученные из текста и метаданных, объединяются и подаются на вход классификатору:
1. Объединенный вектор признаков включает 256 элементов для текста (128*2 из BiLSTM) и 16 элементов для метаданных
2. Классификатор состоит из четырех последовательных блоков с линейными преобразованиями, батч-нормализацией и функцией активации ReLU
3. Размерности промежуточных слоев: 256 → 128 → 64 → 1
4. Применение дропаута с уменьшающейся вероятностью для предотвращения переобучения
5. Финальный выход преобразуется в вероятность с помощью сигмоидной функции

Такая архитектура позволяет эффективно обрабатывать как текстовую информацию, так и метаданные пользователя, что повышает точность определения депрессивного контекста.

## Требования

Полный список зависимостей доступен в файле `requirements.txt` в корне проекта.

Основные зависимости включают:
- PyTorch (>=1.9.0)
- NumPy (>=1.19.5)
- FastText (>=0.9.2)
- Pymorphy3 (>=1.0.0)
- NLTK (>=3.6.2)
- Scikit-learn (>=0.24.2)
- Matplotlib (>=3.4.3)
- WordCloud (>=1.8.1)
- Imbalanced-learn (>=0.8.0) 